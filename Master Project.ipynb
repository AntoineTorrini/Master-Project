{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Project\n",
    "\n",
    "This notebook contains the totality of code used for my Master Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "I used the `twitterscraper` package with the following query.\n",
    "\n",
    "```\n",
    "twitterscraper volkswagen -bd 2015-09-01 -ed 2015-10-15 -o 01_09_2015-15_10_2015.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tnrange\n",
    "\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Let's open the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.21 s, sys: 13 s, total: 21.2 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    labels = ['fullname', 'html', 'id', 'likes', 'replies', 'retweets', 'text', 'timestamp', 'url', 'user']\n",
    "    arr = np.load('data.npy')\n",
    "    df = pd.DataFrame(arr, columns=labels)\n",
    "except FileNotFoundError:\n",
    "    with open('01_09_2015-15_10_2015.json', 'r') as file:\n",
    "        raw = json.load(file)\n",
    "    df = pd.DataFrame(raw)\n",
    "    np.save('data.npy', df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have collected **914 274** tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets collected: 914274\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of tweets collected: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tweet looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fullname                                       Rhondas Romance\n",
       "html         <p class=\"TweetTextSize js-tweet-text tweet-te...\n",
       "id                                          640675779253280768\n",
       "likes                                                        0\n",
       "replies                                                      0\n",
       "retweets                                                     0\n",
       "text         I liked a @YouTube video http://youtu.be/pmzZb...\n",
       "timestamp                                  2015-09-06T23:59:43\n",
       "url                 /RhosBookReviews/status/640675779253280768\n",
       "user                                           RhosBookReviews\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct our analysis, we will only keep the `id` and the `text` columns. Later on, we might rely on likes, replies and retweets to give importance to tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>640675797720801280</td>\n",
       "      <td>Out here with my twin! @big_euro #socaleuro #V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>640675779253280768</td>\n",
       "      <td>I liked a @YouTube video http://youtu.be/pmzZb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>640675589251330048</td>\n",
       "      <td>Triste que ahora estacionen un Volkswagen que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>640675540794515456</td>\n",
       "      <td>I liked a @YouTube video http://youtu.be/D-He3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>640675381922656256</td>\n",
       "      <td>I liked a @YouTube video http://youtu.be/8ZhGp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text\n",
       "0  640675797720801280  Out here with my twin! @big_euro #socaleuro #V...\n",
       "1  640675779253280768  I liked a @YouTube video http://youtu.be/pmzZb...\n",
       "2  640675589251330048  Triste que ahora estacionen un Volkswagen que ...\n",
       "3  640675540794515456  I liked a @YouTube video http://youtu.be/D-He3...\n",
       "4  640675381922656256  I liked a @YouTube video http://youtu.be/8ZhGp..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['fullname', 'html', 'likes', 'replies', 'retweets', 'timestamp', 'url', 'user'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning and removing non english tweets\n",
    "\n",
    "We removed the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "I liked a @YouTube video http://youtu.be/pmzZbUioFAQ?a  2015 Volkswagen Sales Event | “Model Rear End” Passat Commercial\n",
      "Volkswagen pic.twitter.com/hPnvGJ3Ysi\n",
      "Transformed:\n",
      "I liked a @YouTube video   2015 Volkswagen Sales Event | “Model Rear End” Passat Commercial\n",
      "Volkswagen \n"
     ]
    }
   ],
   "source": [
    "print('Original:')\n",
    "print(df['text'][1])\n",
    "print(df['text'][18])\n",
    "print('Transformed:')\n",
    "print(re.sub(r'(https?:\\/\\/|pic.twitter)[^\\s]+', '', df['text'][1]))\n",
    "print(re.sub(r'(https?:\\/\\/|pic.twitter)[^\\s]+', '', df['text'][18]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove the user tags or hastags but not the text associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Out here with my twin! @big_euro #socaleuro #Volkswagen #b7passat #passat #static #bagride… http://ift.tt/1JZMPmk pic.twitter.com/mVowUAs7Xv\n",
      "Transformed:\n",
      "Out here with my twin! big_euro socaleuro Volkswagen b7passat passat static bagride… http://ift.tt/1JZMPmk pic.twitter.com/mVowUAs7Xv\n"
     ]
    }
   ],
   "source": [
    "print('Original:')\n",
    "print(df['text'][0])\n",
    "print('Transformed:')\n",
    "print(re.sub(r'(#|@)', '', df['text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that sums up all the cleaning.\n",
    "\n",
    "We also write a function to keep only the english tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df):\n",
    "    # Create a copy\n",
    "    df_tmp = df.copy()\n",
    "    \n",
    "    # Links cleaning\n",
    "    df_tmp['text'] = df_tmp['text'].str.replace(r'(https?:\\/\\/|pic.twitter)[^\\s]+', '')\n",
    "    \n",
    "    # @ and # cleaning\n",
    "    df_tmp['text'] = df_tmp['text'].str.replace(r'(#|@)', '')\n",
    "    \n",
    "    # Remove the numbers\n",
    "    df_tmp['text'] = df_tmp['text'].str.replace(r'\\d+', '')\n",
    "    \n",
    "    # Put everything to lower case\n",
    "    df_tmp['text'] = df_tmp['text'].str.lower()\n",
    "    \n",
    "    # Punctuation\n",
    "    df_tmp['text'] = df_tmp['text'].str.replace(r'[^\\w\\s]', ' ')\n",
    "    \n",
    "    # Strip whitespaces\n",
    "    df_tmp['text'] = df_tmp['text'].str.replace(r'\\s{2,}', ' ')\n",
    "    \n",
    "    return df_tmp\n",
    "\n",
    "def english_keeper(df):    \n",
    "    english_tweets = []\n",
    "    \n",
    "    for i in tnrange(len(df)):\n",
    "        text = df['text'].iloc[i]\n",
    "        try:\n",
    "            if detect(text) == 'en':\n",
    "                english_tweets.append((df['id'].iloc[i], text))\n",
    "        except LangDetectException:\n",
    "            pass\n",
    "    \n",
    "    df_english = pd.DataFrame.from_records(english_tweets, columns=['id', 'text'])\n",
    "    \n",
    "    return df_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ba09ec173a460aa10c24f5202303f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=27), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from index 0 to 33862\n",
      "Loaded from index 33862 to 67724\n",
      "Loaded from index 67724 to 101586\n",
      "Loaded from index 101586 to 135448\n",
      "Loaded from index 135448 to 169310\n",
      "Loaded from index 169310 to 203172\n",
      "Loaded from index 203172 to 237034\n",
      "Loaded from index 237034 to 270896\n",
      "Loaded from index 270896 to 304758\n",
      "Loaded from index 304758 to 338620\n",
      "Loaded from index 338620 to 372482\n",
      "Loaded from index 372482 to 406344\n",
      "Loaded from index 406344 to 440206\n",
      "Loaded from index 440206 to 474068\n",
      "Loaded from index 474068 to 507930\n",
      "Loaded from index 507930 to 541792\n",
      "Loaded from index 541792 to 575654\n",
      "Loaded from index 575654 to 609516\n",
      "Loaded from index 609516 to 643378\n",
      "Loaded from index 643378 to 677240\n",
      "Loaded from index 677240 to 711102\n",
      "Loaded from index 711102 to 744964\n",
      "Loaded from index 744964 to 778826\n",
      "Loaded from index 778826 to 812688\n",
      "Loaded from index 812688 to 846550\n",
      "Loaded from index 846550 to 880412\n",
      "Loaded from index 880412 to 914274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splits = np.split(df, 27)\n",
    "clean_splits = []\n",
    "labels = ['id', 'text']\n",
    "\n",
    "for i in tnrange(len(splits)):\n",
    "    try:\n",
    "        arr = np.load(f'Processing/split_{i}.npy')\n",
    "        clean_splits.append(pd.DataFrame(arr, columns=['id', 'text']))\n",
    "        print(f'Loaded from index {splits[i].index._start} to {splits[i].index._stop}')\n",
    "    except FileNotFoundError:\n",
    "        print(f'Cleaning from index {splits[i].index._start} to {splits[i].index._stop}')\n",
    "        df_clean = cleaner(splits[i])\n",
    "        df_english = english_keeper(df_clean)\n",
    "        clean_splits.append(df_english)\n",
    "        np.save(f'Processing/split_{i}.npy', df_english.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this cleaning, we loose **62%** of the database, resulting in **339 367** tweets remaining. It does insure a better quality of tweets and will reduce computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339367, 2)\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.concat(clean_splits)\n",
    "print(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    out here with my twin big_euro socaleuro volks...\n",
       "1    i liked a youtube video volkswagen sales event...\n",
       "2    hoje eu to feliiiiiz hoje eu to contente o o f...\n",
       "3    i liked a youtube video volkswagen sales event...\n",
       "4    what an awesome way to display our dealer sign...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "what an awesome way to display our dealer sign just awesome aircooled_ch new volkswagen sign by \n",
      "Transformed:\n",
      "awesome way display dealer sign awesome aircooled_ch new volkswagen sign\n"
     ]
    }
   ],
   "source": [
    "print('Original:')\n",
    "print(df_clean.iloc[4, 1])\n",
    "print('Transformed:')\n",
    "print(' '.join([w for w in df_clean.iloc[4, 1].split(' ') if w not in english_stopwords]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, add=None):\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    \n",
    "    if type(add) == list:\n",
    "        for e in add:\n",
    "            english_stopwords.append(e)\n",
    "            \n",
    "    words_tweets = []\n",
    "    \n",
    "    for i in tnrange(len(df)):\n",
    "        text = ' '.join([w for w in df['text'].iloc[i].split(' ') if w not in english_stopwords]).strip()\n",
    "        words_tweets.append((df['id'].iloc[i], text))\n",
    "\n",
    "    df_words = pd.DataFrame.from_records(words_tweets, columns=['id', 'text'])\n",
    "    \n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e960eb7194e46db9a37ca10ff14bf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_words = remove_stopwords(df_clean, add=['volkswagen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "bringing new tiguan frankfurt filed plants manufacturing frankfurt motor show vol car\n",
      "Transformed:\n",
      "bring new tiguan frankfurt file plant manufactur frankfurt motor show vol car\n"
     ]
    }
   ],
   "source": [
    "sno = SnowballStemmer('english')\n",
    "\n",
    "print('Original:')\n",
    "print(df_words.iloc[13, 1])\n",
    "print('Transformed:')\n",
    "print(' '.join([sno.stem(w) for w in df_words.iloc[13, 1].split(' ')]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(df):\n",
    "    sno = SnowballStemmer('english')\n",
    "    \n",
    "    stemmed_tweets = []\n",
    "    \n",
    "    for i in tnrange(len(df)):\n",
    "        text = ' '.join([sno.stem(w) for w in df['text'].iloc[i].split(' ')]).strip()\n",
    "        stemmed_tweets.append((df['id'].iloc[i], text))\n",
    "\n",
    "    df_stem = pd.DataFrame.from_records(stemmed_tweets, columns=['id', 'text'])\n",
    "    \n",
    "    return df_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c178b27f0c4b919671f1043fd7cde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=339367), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stem = stemming(df_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0         twin big_euro socaleuro bpassat passat static ...\n",
       "1         like youtub video sale event rear passat commerci\n",
       "2         hoje eu feliiiiiz hoje eu content fusca vw vwa...\n",
       "3          like youtub video sale event deal jetta commerci\n",
       "4         awesom way display dealer sign awesom aircoole...\n",
       "5         like youtub video sale event like hot passat c...\n",
       "6            golf want car formal attitud advanc featur ncn\n",
       "7         bet long term russian growth new engin plant m...\n",
       "8         say believ superbeetl one greatest car ever built\n",
       "9         could car want kind car kia picanto aw mini co...\n",
       "10        kombi vw vwar vwair aircool splitbuss splitwin...\n",
       "11        gerherbert optical_laura work neil drive demis...\n",
       "12                                           theyoung jetta\n",
       "13        bring new tiguan frankfurt file plant manufact...\n",
       "14        carsal vwbeetl beetl classic convert vw beetl ...\n",
       "15        chaotic line gtibythewat water vw gti ride cal...\n",
       "16        anyon seen old ladi rear end sale event commer...\n",
       "17               follow linkedin compani page vw sterlingva\n",
       "18                              check mplace download today\n",
       "19                                        think go tri work\n",
       "20                      bring new tiguan frankfurt car news\n",
       "21        photo love sp treffensouth yesterday cool quir...\n",
       "22        want drive beatl satisfact peopl punch eachoth...\n",
       "23        like youtub video askavwsalesguy fix v outlet ...\n",
       "24                  josephcaptur chevi camaro get get jetta\n",
       "25              manag reshuffl creat momentum chang analyst\n",
       "26                 goal subaru wrx sti hatchback type sedan\n",
       "27                                   photo new post publish\n",
       "28        beetl classic red black classic car beetl bid ...\n",
       "29                                       lillicasey get get\n",
       "                                ...                        \n",
       "339337                              auto softwar focus flap\n",
       "339338      michael horn charg vw prosecut fraud arrog stun\n",
       "339339    reel dieselg emiss scandal vw crisi reverber b...\n",
       "339340    accus cooper brazilian militari dictatorship busi\n",
       "339341    admit million diesel car sneaki softwar instal...\n",
       "339342                          bbcnew hit multipl probe us\n",
       "339343    bbc fall vw emiss test worldwid fall fake dies...\n",
       "339344    give custom greater perform greater mileag low...\n",
       "339345    analysi need explain away softwar avoid crimin...\n",
       "339346      rank among world respons compani review rt busi\n",
       "339347    analysi need explain away softwar avoid crimin...\n",
       "339348    unassum engin expos money bullion gold news lo...\n",
       "339349    analysi need explain away softwar avoid crimin...\n",
       "339350                          ceo apolog cheat emiss test\n",
       "339351                      recent ad jetta inventori check\n",
       "339352    need explain away softwar avoid crimin charg e...\n",
       "339353                      justin trudeau thing learn sept\n",
       "339354                      justin trudeau thing learn sept\n",
       "339355    stock market tumbl overnight led german stock ...\n",
       "339356    need explain away softwar avoid crimin charg e...\n",
       "339357      emiss test rig akin captain kirk kobayashi maru\n",
       "339358        say million car worldwid affect diesel decept\n",
       "339359    daili driver cc\\nfile video sedan luxuri senio...\n",
       "339360    winterkorn person deepli sorri broken trust cu...\n",
       "339361    winterkorn person deepli sorri broken trust cu...\n",
       "339362    tt schweiz applemusicfestiv rtsinfraroug lewan...\n",
       "339363                            insid awkward parti world\n",
       "339364    need explain away softwar avoid crimin charg e...\n",
       "339365                 million car use decept emiss softwar\n",
       "339366                    alloc bn cover loss emiss scandal\n",
       "Name: text, Length: 339367, dtype: object>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem['text'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'winterkorn person deepli sorri broken trust custom jdpower carnew'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem.iloc[339361, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like youtub video sale event rear passat commerci'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stem.iloc[1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
